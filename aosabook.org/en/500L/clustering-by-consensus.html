<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="provenance" content="$Id: index.html 1472 2012-09-21 22:17:41Z audrey $" />
    <link rel="stylesheet" href="theme/css/bootstrap.css" type="text/css" />
    <link rel="stylesheet" href="theme/css/bootstrap-responsive.css" type="text/css" />
    <link rel="stylesheet" href="theme/css/code.css" type="text/css" />
    <link rel="stylesheet" href="theme/css/500L.css" type="text/css" />
    <title>500 Lines or Less | Clustering by Consensus</title>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          },
        });
    </script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
</head>
  <body>
    <div class="container">
      <div class="row">
        <div class="hero-unit">
	  <a class='pull-right' href='../index.html'></a>
          <h1>Clustering by Consensus</h1>
          <h2 class="author">Dustin J. Mitchell</h2>
          <blockquote class="pull-right">
            </blockquote>

        </div>
      </div>
      <div class="row">
        <div class='span10 offset1' id='content'>
          <p><em>Dustin is an open source software developer and release engineer at Mozilla. He has worked on projects as varied as a host configuration system in Puppet, a Flask-based web framework, unit tests for firewall configurations, and a continuous integration framework in Twisted Python. Find him as <a href="http://github.com/djmitche">@djmitche</a> on GitHub or at <script type="text/javascript">
<!--
h='&#x6d;&#x6f;&#122;&#x69;&#108;&#108;&#x61;&#46;&#x63;&#x6f;&#x6d;';a='&#64;';n='&#100;&#x75;&#x73;&#116;&#x69;&#110;';e=n+a+h;
document.write('<a h'+'ref'+'="ma'+'ilto'+':'+e+'">'+e+'<\/'+'a'+'>');
// -->
</script><noscript>&#100;&#x75;&#x73;&#116;&#x69;&#110;&#32;&#x61;&#116;&#32;&#x6d;&#x6f;&#122;&#x69;&#108;&#108;&#x61;&#32;&#100;&#x6f;&#116;&#32;&#x63;&#x6f;&#x6d;</noscript>.</em></p>

<h2 id="introduction">Introduction</h2>

<p>In this chapter, we'll explore implementation of a network protocol designed to support reliable distributed computation. Network protocols can be difficult to implement correctly, so we'll look at some techniques for minimizing bugs and for catching and fixing the remaining few. Building reliable software, too, requires some special development and debugging techniques.</p>

<h2 id="motivating-example">Motivating Example</h2>

<p>The focus of this chapter is on the protocol implementation, but as a motivating example let's consider a simple bank account management service. In this service, each account has a current balance and is identified with an account number. Users access the accounts by requesting operations like &quot;deposit&quot;, &quot;transfer&quot;, or &quot;get-balance&quot;. The &quot;transfer&quot; operation operates on two accounts at once -- the source and destination accounts -- and must be rejected if the source account's balance is too low.</p>

<p>If the service is hosted on a single server, this is easy to implement: use a lock to make sure that transfer operations don't run in parallel, and verify the source account's balance in that method. However, a bank cannot rely on a single server for its critical account balances. Instead, the service is <em>distributed</em> over multiple servers, with each running a separate instance of exactly the same code. Users can then contact any server to perform an operation.</p>

<p>In a naive implementation of distributed processing, each server would keep a local copy of every account's balance. It would handle any operations it received, and send updates for account balances to other servers. But this approach introduces a serious failure mode: if two servers process operations for the same account at the same time, which new account balance is correct? Even if the servers share operations with one another instead of balances, two simultaneous transfers out of an account might overdraw the account.</p>

<p>Fundamentally, these failures occur when servers use their local state to perform operations, without first ensuring that the local state matches the state on other servers. For example, imagine that server A receives a transfer operation from Account 101 to Account 202, when server B has already processed another transfer of Account 101's full balance to Account 202, but not yet informed server A. The local state on server A is different from that on server B, so server A incorrectly allows the transfer to complete, even though the result is an overdraft on Account 101.</p>

<h2 id="distributed-state-machines">Distributed State Machines</h2>

<p>The technique for avoiding such problems is called a &quot;distributed state machine&quot;. The idea is that each server executes exactly the same deterministic state machine on exactly the same inputs. By the nature of state machines, then, each server will see exactly the same outputs. Operations such as &quot;transfer&quot; or &quot;get-balance&quot;, together with their parameters (account numbers and amounts) represent the inputs to the state machine.</p>

<p>The state machine for this application is simple:</p>

<pre class="sourceCode python"><code class="sourceCode python">    <span class="kw">def</span> execute_operation(state, operation):
        <span class="kw">if</span> operation.name == <span class="st">&#39;deposit&#39;</span>:
            <span class="kw">if</span> not verify_signature(operation.deposit_signature):
                <span class="kw">return</span> state, <span class="ot">False</span>
            state.accounts[operation.destination_account] += operation.amount
            <span class="kw">return</span> state, <span class="ot">True</span>
        <span class="kw">elif</span> operation.name == <span class="st">&#39;transfer&#39;</span>:
            <span class="kw">if</span> state.accounts[operation.source_account] &lt; operation.amount:
                <span class="kw">return</span> state, <span class="ot">False</span>
            state.accounts[operation.source_account] -= operation.amount
            state.accounts[operation.destination_account] += operation.amount
            <span class="kw">return</span> state, <span class="ot">True</span>
        <span class="kw">elif</span> operation.name == <span class="st">&#39;get-balance&#39;</span>:
            <span class="kw">return</span> state, state.accounts[operation.account]</code></pre>

<p>Note that executing the &quot;get-balance&quot; operation does not modify the state, but is still implemented as a state transition. This guarantees that the returned balance is the latest information in the cluster of servers, and is not based on the (possibly stale) local state on a single server.</p>

<p>This may look different than the typical state machine you'd learn about in a computer science course. Rather than a finite set of named states with labeled transitions, this machine's state is the collection of account balances, so there are infinite possible states. Still, the usual rules of deterministic state machines apply: starting with the same state and processing the same operations will always produce the same output.</p>

<p>So, the distributed state machine technique ensures that the same operations occur on each host. But the problem remains of ensuring that every server agrees on the inputs to the state machine. This is a problem of <em>consensus</em>, and we'll address it with a derivative of the Paxos algorithm.</p>

<h2 id="consensus-by-paxos">Consensus by Paxos</h2>

<p>Paxos was described by Leslie Lamport in a fanciful paper, first submitted in 1990 and eventually published in 1998, entitled &quot;The Part-Time Parliament&quot;<a href="clustering-by-consensus.html#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. Lamport's paper has a great deal more detail than we will get into here, and is a fun read. The references at the end of the chapter describe some extensions of the algorithm that we have adapted in this implementation.</p>

<p>The simplest form of Paxos provides a way for a set of servers to agree on one value, for all time. Multi-Paxos builds on this foundation by agreeing on a numbered sequence of facts, one at a time. To implement a distributed state machine, we use Multi-Paxos to agree on each state-machine input, and execute them in sequence.</p>

<h3 id="simple-paxos">Simple Paxos</h3>

<p>So let's start with &quot;Simple Paxos&quot;, also known as the Synod protocol, which provides a way to agree on a single value that can never change. The name Paxos comes from the mythical island in &quot;The Part-Time Parliament&quot;, where lawmakers vote on legislation through a process Lamport dubbed the Synod protocol.</p>

<p>The algorithm is a building block for more complex algorithms, as we'll see below. The single value we'll agree on in this example is the first transaction processed by our hypothetical bank. While the bank will process transactions every day, the first transaction will only occur once and never change, so we can use Simple Paxos to agree on its details.</p>

<p>The protocol operates in a series of ballots, each led by a single member of the cluster, called the proposer. Each ballot has a unique ballot number based on an integer and the proposer's identity. The proposer's goal is to get a majority of cluster members, acting as acceptors, to accept its value, but only if another value has not already been decided.</p>

<div class="center figure">
<a name="figure-3.1"></a><img src="cluster-images/ballot.png" alt="Figure 3.1 - A Ballot" title="Figure 3.1 - A Ballot" />
</div>

<p class="center figcaption">
<small>Figure 3.1 - A Ballot</small>
</p>

<p>A ballot begins with the proposer sending a <code>Prepare</code> message with the ballot number <em>N</em> to the acceptors and waiting to hear from a majority (<a href="clustering-by-consensus.html#figure-3.1">Figure 3.1</a>.)</p>

<p>The <code>Prepare</code> message is a request for the accepted value (if any) with the highest ballot number less than <em>N</em>. Acceptors respond with a <code>Promise</code> containing any value they have already accepted, and promising not to accept any ballot numbered less than <em>N</em> in the future. If the acceptor has already made a promise for a larger ballot number, it includes that number in the <code>Promise</code>, indicating that the proposer has been pre-empted. In this case, the ballot is over, but the proposer is free to try again in another ballot (and with a larger ballot number).</p>

<p>When the proposer has heard back from a majority of the acceptors, it sends an <code>Accept</code> message, including the ballot number and value, to all acceptors. If the proposer did not receive any existing value from any acceptor, then it sends its own desired value. Otherwise, it sends the value from the highest-numbered promise.</p>

<p>Unless it would violate a promise, each acceptor records the value from the <code>Accept</code> message as accepted and replies with an <code>Accepted</code> message. The ballot is complete and the value decided when the proposer has heard its ballot number from a majority of acceptors.</p>

<p>Returning to the example, initially no other value has been accepted, so the acceptors all send back a <code>Promise</code> with no value, and the proposer sends an <code>Accept</code> containing its value, say:</p>

<pre class="sourceCode python"><code class="sourceCode python">    operation(name=<span class="st">&#39;deposit&#39;</span>, amount=<span class="fl">100.00</span>, destination_account=<span class="st">&#39;Mike DiBernardo&#39;</span>)</code></pre>

<p>If another proposer later initiates a ballot with a lower ballot number and a different operation (say, a transfer to acount <code>'Dustin J. Mitchell'</code>), the acceptors will simply not accept it. If that ballot has a larger ballot number, then the <code>Promise</code> from the acceptors will inform the proposer about Michael's $100.00 deposit operation, and the proposer will send that value in the <code>Accept</code> message instead of the transfer to Dustin. The new ballot will be accepted, but in favor of the same value as the first ballot.</p>

<p>In fact, the protocol will never allow two different values to be decided, even if the ballots overlap, messages are delayed, or a minority of acceptors fail.</p>

<p>When multiple proposers make a ballot at the same time, it is easy for neither ballot to be accepted. Both proposers then re-propose, and hopefully one wins, but the deadlock can continue indefinitely if the timing works out just right.</p>

<p>Consider the following sequence of events:</p>

<ul>
<li>Proposer A performs the <code>Prepare</code>/<code>Promise</code> phase for ballot number 1.</li>
<li>Before Proposer A manages to get its proposal accepted, Proposer B performs a <code>Prepare</code>/<code>Promise</code> phase for ballot number 2.</li>
<li>When Proposer A finally sends its <code>Accept</code> with ballot number 1, the acceptors reject it because they have already promised ballot number 2.</li>
<li>Proposer A reacts by immediately sending a <code>Prepare</code> with a higher ballot number (3), before Proposer B can send its <code>Accept</code> message.</li>
<li>Proposer B's subsequent <code>Accept</code> is rejected, and the process repeats.</li>
</ul>

<p>With unlucky timing -- more common over long-distance connections where the time between sending a message and getting a response is long -- this deadlock can continue for many rounds.</p>

<h3 id="multi-paxos">Multi-Paxos</h3>

<p>Reaching consensus on a single static value is not particularly useful on its own. Clustered systems such as the bank account service want to agree on a particular state (account balances) that changes over time. We use Paxos to agree on each operation, treated as a state machine transition.</p>

<p>Multi-Paxos is, in effect, a sequence of simple Paxos instances (slots), each numbered sequentially. Each state transition is given a &quot;slot number&quot;, and each member of the cluster executes transitions in strict numeric order. To change the cluster's state (to process a transfer operation, for example), we try to achieve consensus on that operation in the next slot. In concrete terms, this means adding a slot number to each message, with all of the protocol state tracked on a per-slot basis.</p>

<p>Running Paxos for every slot, with its minimum of two round trips, would be too slow. Multi-Paxos optimizes by using the same set of ballot numbers for all slots, and performing the <code>Prepare</code>/<code>Promise</code> phase for all slots at once.</p>

<h3 id="paxos-made-pretty-hard">Paxos Made Pretty Hard</h3>

<p>Implementing Multi-Paxos in practical software is notoriously difficult, spawning a number of papers mocking Lamport's &quot;Paxos Made Simple&quot; with titles like &quot;Paxos Made Practical&quot;.</p>

<p>First, the multiple-proposers problem described above can become problematic in a busy environment, as each cluster member attempts to get its state machine operation decided in each slot. The fix is to elect a &quot;leader&quot; which is responsible for submitting ballots for each slot. All other cluster nodes then send new operations to the leader for execution. Thus, in normal operation with only one leader, ballot conflicts do not occur.</p>

<p>The <code>Prepare</code>/<code>Promise</code> phase can function as a kind of leader election: whichever cluster member owns the most recently promised ballot number is considered the leader. The leader is then free to execute the <code>Accept</code>/<code>Accepted</code> phase directly without repeating the first phase. As we'll see below, leader elections are actually quite complex.</p>

<p>Although simple Paxos guarantees that the cluster will not reach conflicting decisions, it cannot guarantee that any decision will be made. For example, if the initial <code>Prepare</code> message is lost and doesn't reach the acceptors, then the proposer will wait for a <code>Promise</code> message that will never arrive. Fixing this requires carefully orchestrated re-transmissions: enough to eventually make progress, but not so many that the cluster buries itself in a packet storm.</p>

<p>Another problem is the dissemination of decisions. A simple broadcast of a <code>Decision</code> message can take care of this for the normal case. If the message is lost, though, a node can remain permanently ignorant of the decision and unable to apply state machine transitions for later slots. So an implementation needs some mechanism for sharing information about decided proposals.</p>

<p>Our use of a distributed state machine presents another interesting challenge: start-up. When a new node starts, it needs to catch up on the existing state of the cluster. Although it can do so by catching up on decisions for all slots since the first, in a mature cluster this may involve millions of slots. Furthermore, we need some way to initialize a new cluster.</p>

<p>But enough talk of theory and algorithms -- let's have a look at the code.</p>

<h2 id="introducing-cluster">Introducing Cluster</h2>

<p>The <em>Cluster</em> library in this chapter implements a simple form of Multi-Paxos. It is designed as a library to provide a consensus service to a larger application.</p>

<p>Users of this library will depend on its correctness, so it's important to structure the code so that we can see -- and test -- its correspondence to the specification. Complex protocols can exhibit complex failures, so we will build support for reproducing and debugging rare failures.</p>

<p>The implementation in this chapter is proof-of-concept code: enough to demonstrate that the core concept is practical, but without all of the mundane equipment required for use in production. The code is structured so that such equipment can be added later with minimal changes to the core implementation.</p>

<p>Let's get started.</p>

<h3 id="types-and-constants">Types and Constants</h3>

<p>Cluster's protocol uses fifteen different message types, each defined as a Python <a href="https://docs.python.org/3/library/collections.html"><code>namedtuple</code></a>.</p>

<pre class="sourceCode python"><code class="sourceCode python">    Accepted = namedtuple(<span class="st">&#39;Accepted&#39;</span>, [<span class="st">&#39;slot&#39;</span>, <span class="st">&#39;ballot_num&#39;</span>])
    Accept = namedtuple(<span class="st">&#39;Accept&#39;</span>, [<span class="st">&#39;slot&#39;</span>, <span class="st">&#39;ballot_num&#39;</span>, <span class="st">&#39;proposal&#39;</span>])
    Decision = namedtuple(<span class="st">&#39;Decision&#39;</span>, [<span class="st">&#39;slot&#39;</span>, <span class="st">&#39;proposal&#39;</span>])
    Invoked = namedtuple(<span class="st">&#39;Invoked&#39;</span>, [<span class="st">&#39;client_id&#39;</span>, <span class="st">&#39;output&#39;</span>])
    Invoke = namedtuple(<span class="st">&#39;Invoke&#39;</span>, [<span class="st">&#39;caller&#39;</span>, <span class="st">&#39;client_id&#39;</span>, <span class="st">&#39;input_value&#39;</span>])
    Join = namedtuple(<span class="st">&#39;Join&#39;</span>, [])
    Active = namedtuple(<span class="st">&#39;Active&#39;</span>, [])
    Prepare = namedtuple(<span class="st">&#39;Prepare&#39;</span>, [<span class="st">&#39;ballot_num&#39;</span>])
    Promise = namedtuple(<span class="st">&#39;Promise&#39;</span>, [<span class="st">&#39;ballot_num&#39;</span>, <span class="st">&#39;accepted_proposals&#39;</span>])
    Propose = namedtuple(<span class="st">&#39;Propose&#39;</span>, [<span class="st">&#39;slot&#39;</span>, <span class="st">&#39;proposal&#39;</span>])
    Welcome = namedtuple(<span class="st">&#39;Welcome&#39;</span>, [<span class="st">&#39;state&#39;</span>, <span class="st">&#39;slot&#39;</span>, <span class="st">&#39;decisions&#39;</span>])
    Decided = namedtuple(<span class="st">&#39;Decided&#39;</span>, [<span class="st">&#39;slot&#39;</span>])
    Preempted = namedtuple(<span class="st">&#39;Preempted&#39;</span>, [<span class="st">&#39;slot&#39;</span>, <span class="st">&#39;preempted_by&#39;</span>])
    Adopted = namedtuple(<span class="st">&#39;Adopted&#39;</span>, [<span class="st">&#39;ballot_num&#39;</span>, <span class="st">&#39;accepted_proposals&#39;</span>])
    Accepting = namedtuple(<span class="st">&#39;Accepting&#39;</span>, [<span class="st">&#39;leader&#39;</span>])</code></pre>

<p>Using named tuples to describe each message type keeps the code clean and helps avoid some simple errors. The named tuple constructor will raise an exception if it is not given exactly the right attributes, making typos obvious. The tuples format themselves nicely in log messages, and as an added bonus don't use as much memory as a dictionary.</p>

<p>Creating a message reads naturally:</p>

<pre class="sourceCode python"><code class="sourceCode python">    msg = Accepted(slot=<span class="dv">10</span>, ballot_num=<span class="dv">30</span>)</code></pre>

<p>And the fields of that message are accessible with a minimum of extra typing:</p>

<pre class="sourceCode python"><code class="sourceCode python">    got_ballot_num = msg.ballot_num</code></pre>

<p>We'll see what these messages mean in the sections that follow. The code also introduces a few constants, most of which define timeouts for various messages:</p>

<pre class="sourceCode python"><code class="sourceCode python">    JOIN_RETRANSMIT = <span class="fl">0.7</span>
    CATCHUP_INTERVAL = <span class="fl">0.6</span>
    ACCEPT_RETRANSMIT = <span class="fl">1.0</span>
    PREPARE_RETRANSMIT = <span class="fl">1.0</span>
    INVOKE_RETRANSMIT = <span class="fl">0.5</span>
    LEADER_TIMEOUT = <span class="fl">1.0</span>
    NULL_BALLOT = Ballot(-<span class="dv">1</span>, -<span class="dv">1</span>)  <span class="co"># sorts before all real ballots</span>
    NOOP_PROPOSAL = Proposal(<span class="ot">None</span>, <span class="ot">None</span>, <span class="ot">None</span>)  <span class="co"># no-op to fill otherwise empty slots</span></code></pre>

<p>Finally, Cluster uses two data types named to correspond to the protocol description:</p>

<pre class="sourceCode python"><code class="sourceCode python">    Proposal = namedtuple(<span class="st">&#39;Proposal&#39;</span>, [<span class="st">&#39;caller&#39;</span>, <span class="st">&#39;client_id&#39;</span>, <span class="st">&#39;input&#39;</span>])
    Ballot = namedtuple(<span class="st">&#39;Ballot&#39;</span>, [<span class="st">&#39;n&#39;</span>, <span class="st">&#39;leader&#39;</span>])</code></pre>

<h3 id="component-model">Component Model</h3>

<p>Humans are limited by what we can hold in our active memory. We can't reason about the entire Cluster implementation at once -- it's just too much, so it's easy to miss details. For similar reasons, large monolithic codebases are hard to test: test cases must manipulate many moving pieces and are brittle, failing on almost any change to the code.</p>

<p>To encourage testability and keep the code readable, we break Cluster down into a handful of classes corresponding to the roles described in the protocol. Each is a subclass of <code>Role</code>.</p>

<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">class</span> Role(<span class="dt">object</span>):

    <span class="kw">def</span> <span class="ot">__init__</span>(<span class="ot">self</span>, node):
        <span class="ot">self</span>.node = node
        <span class="ot">self</span>.node.register(<span class="ot">self</span>)
        <span class="ot">self</span>.running = <span class="ot">True</span>
        <span class="ot">self</span>.logger = node.logger.getChild(<span class="dt">type</span>(<span class="ot">self</span>).<span class="ot">__name__</span>)

    <span class="kw">def</span> set_timer(<span class="ot">self</span>, seconds, callback):
        <span class="kw">return</span> <span class="ot">self</span>.node.network.set_timer(<span class="ot">self</span>.node.address, seconds,
                                           <span class="kw">lambda</span>: <span class="ot">self</span>.running and callback())

    <span class="kw">def</span> stop(<span class="ot">self</span>):
        <span class="ot">self</span>.running = <span class="ot">False</span>
        <span class="ot">self</span>.node.unregister(<span class="ot">self</span>)</code></pre>

<p>The roles that a cluster node has are glued together by the <code>Node</code> class, which represents a single node on the network. Roles are added to and removed from the node as execution proceeds. Messages that arrive on the node are relayed to all active roles, calling a method named after the message type with a <code>do_</code> prefix. These <code>do_</code> methods receive the message's attributes as keyword arguments for easy access. The <code>Node</code> class also provides a <code>send</code> method as a convenience, using <code>functools.partial</code> to supply some arguments to the same methods of the <code>Network</code> class.</p>

<pre class="sourceCode python"><code class="sourceCode python">
<span class="kw">class</span> Node(<span class="dt">object</span>):
    unique_ids = itertools.count()

    <span class="kw">def</span> <span class="ot">__init__</span>(<span class="ot">self</span>, network, address):
        <span class="ot">self</span>.network = network
        <span class="ot">self</span>.address = address or <span class="st">&#39;N</span><span class="ot">%d</span><span class="st">&#39;</span> % <span class="ot">self</span>.unique_ids.<span class="dt">next</span>()
        <span class="ot">self</span>.logger = SimTimeLogger(
            logging.getLogger(<span class="ot">self</span>.address), {<span class="st">&#39;network&#39;</span>: <span class="ot">self</span>.network})
        <span class="ot">self</span>.logger.info(<span class="st">&#39;starting&#39;</span>)
        <span class="ot">self</span>.roles = []
        <span class="ot">self</span>.send = functools.partial(<span class="ot">self</span>.network.send, <span class="ot">self</span>)

    <span class="kw">def</span> register(<span class="ot">self</span>, roles):
        <span class="ot">self</span>.roles.append(roles)

    <span class="kw">def</span> unregister(<span class="ot">self</span>, roles):
        <span class="ot">self</span>.roles.remove(roles)

    <span class="kw">def</span> receive(<span class="ot">self</span>, sender, message):
        handler_name = <span class="st">&#39;do_</span><span class="ot">%s</span><span class="st">&#39;</span> % <span class="dt">type</span>(message).<span class="ot">__name__</span>

        <span class="kw">for</span> comp in <span class="ot">self</span>.roles[:]:
            <span class="kw">if</span> not <span class="dt">hasattr</span>(comp, handler_name):
                <span class="kw">continue</span>
            comp.logger.debug(<span class="st">&quot;received </span><span class="ot">%s</span><span class="st"> from </span><span class="ot">%s</span><span class="st">&quot;</span>, message, sender)
            fn = <span class="dt">getattr</span>(comp, handler_name)
            fn(sender=sender, **message._asdict())
    </code></pre>

<h3 id="application-interface">Application Interface</h3>

<p>The application creates and starts a <code>Member</code> object on each cluster member, providing an application-specific state machine and a list of peers. The member object adds a bootstrap role to the node if it is joining an existing cluster, or seed if it is creating a new cluster. It then runs the protocol (via <code>Network.run</code>) in a separate thread.</p>

<p>The application interacts with the cluster through the <code>invoke</code> method, which kicks off a proposal for a state transition. Once that proposal is decided and the state machine runs, <code>invoke</code> returns the machine's output. The method uses a simple synchronized <code>Queue</code> to wait for the result from the protocol thread.</p>

<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">class</span> Member(<span class="dt">object</span>):

    <span class="kw">def</span> <span class="ot">__init__</span>(<span class="ot">self</span>, state_machine, network, peers, seed=<span class="ot">None</span>,
                 seed_cls=Seed, bootstrap_cls=Bootstrap):
        <span class="ot">self</span>.network = network
        <span class="ot">self</span>.node = network.new_node()
        <span class="kw">if</span> seed is not <span class="ot">None</span>:
            <span class="ot">self</span>.startup_role = seed_cls(<span class="ot">self</span>.node, initial_state=seed, peers=peers,
                                      execute_fn=state_machine)
        <span class="kw">else</span>:
            <span class="ot">self</span>.startup_role = bootstrap_cls(<span class="ot">self</span>.node,
                                      execute_fn=state_machine, peers=peers)
        <span class="ot">self</span>.requester = <span class="ot">None</span>

    <span class="kw">def</span> start(<span class="ot">self</span>):
        <span class="ot">self</span>.startup_role.start()
        <span class="ot">self</span>.thread = threading.Thread(target=<span class="ot">self</span>.network.run)
        <span class="ot">self</span>.thread.start()

    <span class="kw">def</span> invoke(<span class="ot">self</span>, input_value, request_cls=Requester):
        <span class="kw">assert</span> <span class="ot">self</span>.requester is <span class="ot">None</span>
        q = Queue.Queue()
        <span class="ot">self</span>.requester = request_cls(<span class="ot">self</span>.node, input_value, q.put)
        <span class="ot">self</span>.requester.start()
        output = q.get()
        <span class="ot">self</span>.requester = <span class="ot">None</span>
        <span class="kw">return</span> output</code></pre>

<h3 id="role-classes">Role Classes</h3>

<p>Let's look at each of the role classes in the library one by one.</p>

<h4 id="acceptor">Acceptor</h4>

<p>The <code>Acceptor</code> implements the acceptor role in the protocol, so it must store the ballot number representing its most recent promise, along with the set of accepted proposals for each slot. It then responds to <code>Prepare</code> and <code>Accept</code> messages according to the protocol. The result is a short class that is easy to compare to the protocol.</p>

<p>For acceptors, Multi-Paxos looks a lot like Simple Paxos, with the addition of slot numbers to the messages.</p>

<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">class</span> Acceptor(Role):

    <span class="kw">def</span> <span class="ot">__init__</span>(<span class="ot">self</span>, node):
        <span class="dt">super</span>(Acceptor, <span class="ot">self</span>).<span class="ot">__init__</span>(node)
        <span class="ot">self</span>.ballot_num = NULL_BALLOT
        <span class="ot">self</span>.accepted_proposals = {}  <span class="co"># {slot: (ballot_num, proposal)}</span>

    <span class="kw">def</span> do_Prepare(<span class="ot">self</span>, sender, ballot_num):
        <span class="kw">if</span> ballot_num &gt; <span class="ot">self</span>.ballot_num:
            <span class="ot">self</span>.ballot_num = ballot_num
            <span class="co"># we&#39;ve heard from a scout, so it might be the next leader</span>
            <span class="ot">self</span>.node.send([<span class="ot">self</span>.node.address], Accepting(leader=sender))

        <span class="ot">self</span>.node.send([sender], Promise(
            ballot_num=<span class="ot">self</span>.ballot_num, 
            accepted_proposals=<span class="ot">self</span>.accepted_proposals
        ))

    <span class="kw">def</span> do_Accept(<span class="ot">self</span>, sender, ballot_num, slot, proposal):
        <span class="kw">if</span> ballot_num &gt;= <span class="ot">self</span>.ballot_num:
            <span class="ot">self</span>.ballot_num = ballot_num
            acc = <span class="ot">self</span>.accepted_proposals
            <span class="kw">if</span> slot not in acc or acc[slot][<span class="dv">0</span>] &lt; ballot_num:
                acc[slot] = (ballot_num, proposal)

        <span class="ot">self</span>.node.send([sender], Accepted(
            slot=slot, ballot_num=<span class="ot">self</span>.ballot_num))</code></pre>

<h4 id="replica">Replica</h4>

<p><a name="sec.cluster.replica"> </a></p>

<p>The <code>Replica</code> class is the most complicated role class, as it has a few closely related responsibilities:</p>

<ul>
<li>Making new proposals;</li>
<li>Invoking the local state machine when proposals are decided;</li>
<li>Tracking the current leader; and</li>
<li>Adding newly started nodes to the cluster.</li>
</ul>

<p>The replica creates new proposals in response to <code>Invoke</code> messages from clients, selecting what it believes to be an unused slot and sending a <code>Propose</code> message to the current leader (<a href="clustering-by-consensus.html#figure-3.2">Figure 3.2</a>.) Furthermore, if the consensus for the selected slot is for a different proposal, the replica must re-propose with a new slot.</p>

<div class="center figure">
<a name="figure-3.2"></a><img src="cluster-images/replica.png" alt="Figure 3.2 - Replica Role Control Flow" title="Figure 3.2 - Replica Role Control Flow" />
</div>

<p class="center figcaption">
<small>Figure 3.2 - Replica Role Control Flow</small>
</p>

<p><code>Decision</code> messages represent slots on which the cluster has come to consensus. Here, replicas store the new decision, then run the state machine until it reaches an undecided slot. Replicas distinguish <em>decided</em> slots, on which the cluster has agreed, from <em>committed</em> slots, which the local state machine has processed. When slots are decided out of order, the committed proposals may lag behind, waiting for the next slot to be decided. When a slot is committed, each replica sends an <code>Invoked</code> message back to the requester with the result of the operation.</p>

<p>In some circumstances, it's possible for a slot to have no active proposals and no decision. The state machine is required to execute slots one by one, so the cluster must reach a consensus on something to fill the slot. To protect against this possibility, replicas make a &quot;no-op&quot; proposal whenever they catch up on a slot. If such a proposal is eventually decided, then the state machine does nothing for that slot.</p>

<p>Likewise, it's possible for the same proposal to be decided twice. The replica skips invoking the state machine for any such duplicate proposals, performing no transition for that slot.</p>

<p>Replicas need to know which node is the active leader in order to send <code>Propose</code> messages to it. There is a surprising amount of subtlety required to get this right, as we'll see later. Each replica tracks the active leader using three sources of information.</p>

<p>When the leader role becomes active, it sends an <code>Adopted</code> message to the replica on the same node (<a href="clustering-by-consensus.html#figure-3.3">Figure 3.3</a>.)</p>

<div class="center figure">
<a name="figure-3.3"></a><img src="cluster-images/adopted.png" alt="Figure 3.3 - Adopted" title="Figure 3.3 - Adopted" />
</div>

<p class="center figcaption">
<small>Figure 3.3 - Adopted</small>
</p>

<p>When the acceptor role sends a <code>Promise</code> to a new leader, it sends an <code>Accepting</code> message to its local replica (<a href="clustering-by-consensus.html#figure-3.4">Figure 3.4</a>.)</p>

<div class="center figure">
<a name="figure-3.4"></a><img src="cluster-images/accepting.png" alt="Figure 3.4 - Accepting" title="Figure 3.4 - Accepting" />
</div>

<p class="center figcaption">
<small>Figure 3.4 - Accepting</small>
</p>

<p>The active leader sends <code>Active</code> messages as a heartbeat (<a href="clustering-by-consensus.html#figure-3.5">Figure 3.5</a>.) If no such message arrives before the <code>LEADER_TIMEOUT</code> expires, the replica assumes the leader is dead and moves on to the next leader. In this case, it's important that all replicas choose the <em>same</em> new leader, which we accomplish by sorting the members and selecting the next one in the list.</p>

<div class="center figure">
<a name="figure-3.5"></a><img src="cluster-images/active.png" alt="Figure 3.5 - Active" title="Figure 3.5 - Active" />
</div>

<p class="center figcaption">
<small>Figure 3.5 - Active</small>
</p>

<p>Finally, when a node joins the network, the bootstrap role sends a <code>Join</code> message (<a href="clustering-by-consensus.html#figure-3.6">Figure 3.6</a>.) The replica responds with a <code>Welcome</code> message containing its most recent state, allowing the new node to come up to speed quickly.</p>

<div class="center figure">
<a name="figure-3.6"></a><img src="cluster-images/bootstrap.png" alt="Figure 3.6 - Bootstrap" title="Figure 3.6 - Bootstrap" />
</div>

<p class="center figcaption">
<small>Figure 3.6 - Bootstrap</small>
</p>

<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">class</span> Replica(Role):

    <span class="kw">def</span> <span class="ot">__init__</span>(<span class="ot">self</span>, node, execute_fn, state, slot, decisions, peers):
        <span class="dt">super</span>(Replica, <span class="ot">self</span>).<span class="ot">__init__</span>(node)
        <span class="ot">self</span>.execute_fn = execute_fn
        <span class="ot">self</span>.state = state
        <span class="ot">self</span>.slot = slot
        <span class="ot">self</span>.decisions = decisions
        <span class="ot">self</span>.peers = peers
        <span class="ot">self</span>.proposals = {}
        <span class="co"># next slot num for a proposal (may lead slot)</span>
        <span class="ot">self</span>.next_slot = slot
        <span class="ot">self</span>.latest_leader = <span class="ot">None</span>
        <span class="ot">self</span>.latest_leader_timeout = <span class="ot">None</span>

    <span class="co"># making proposals</span>

    <span class="kw">def</span> do_Invoke(<span class="ot">self</span>, sender, caller, client_id, input_value):
        proposal = Proposal(caller, client_id, input_value)
        slot = <span class="dt">next</span>((s <span class="kw">for</span> s, p in <span class="ot">self</span>.proposals.iteritems() <span class="kw">if</span> p == proposal), <span class="ot">None</span>)
        <span class="co"># propose, or re-propose if this proposal already has a slot</span>
        <span class="ot">self</span>.propose(proposal, slot)

    <span class="kw">def</span> propose(<span class="ot">self</span>, proposal, slot=<span class="ot">None</span>):
        <span class="co">&quot;&quot;&quot;Send (or resend, if slot is specified) a proposal to the leader&quot;&quot;&quot;</span>
        <span class="kw">if</span> not slot:
            slot, <span class="ot">self</span>.next_slot = <span class="ot">self</span>.next_slot, <span class="ot">self</span>.next_slot + <span class="dv">1</span>
        <span class="ot">self</span>.proposals[slot] = proposal
        <span class="co"># find a leader we think is working - either the latest we know of, or</span>
        <span class="co"># ourselves (which may trigger a scout to make us the leader)</span>
        leader = <span class="ot">self</span>.latest_leader or <span class="ot">self</span>.node.address
        <span class="ot">self</span>.logger.info(
            <span class="st">&quot;proposing </span><span class="ot">%s</span><span class="st"> at slot </span><span class="ot">%d</span><span class="st"> to leader </span><span class="ot">%s</span><span class="st">&quot;</span> % (proposal, slot, leader))
        <span class="ot">self</span>.node.send([leader], Propose(slot=slot, proposal=proposal))

    <span class="co"># handling decided proposals</span>

    <span class="kw">def</span> do_Decision(<span class="ot">self</span>, sender, slot, proposal):
        <span class="kw">assert</span> not <span class="ot">self</span>.decisions.get(<span class="ot">self</span>.slot, <span class="ot">None</span>), \
                <span class="co">&quot;next slot to commit is already decided&quot;</span>
        <span class="kw">if</span> slot in <span class="ot">self</span>.decisions:
            <span class="kw">assert</span> <span class="ot">self</span>.decisions[slot] == proposal, \
                <span class="co">&quot;slot %d already decided with %r!&quot;</span> % (slot, <span class="ot">self</span>.decisions[slot])
            <span class="kw">return</span>
        <span class="ot">self</span>.decisions[slot] = proposal
        <span class="ot">self</span>.next_slot = <span class="dt">max</span>(<span class="ot">self</span>.next_slot, slot + <span class="dv">1</span>)

        <span class="co"># re-propose our proposal in a new slot if it lost its slot and wasn&#39;t a no-op</span>
        our_proposal = <span class="ot">self</span>.proposals.get(slot)
        <span class="kw">if</span> (our_proposal is not <span class="ot">None</span> and 
            our_proposal != proposal and our_proposal.caller):
            <span class="ot">self</span>.propose(our_proposal)

        <span class="co"># execute any pending, decided proposals</span>
        <span class="kw">while</span> <span class="ot">True</span>:
            commit_proposal = <span class="ot">self</span>.decisions.get(<span class="ot">self</span>.slot)
            <span class="kw">if</span> not commit_proposal:
                <span class="kw">break</span>  <span class="co"># not decided yet</span>
            commit_slot, <span class="ot">self</span>.slot = <span class="ot">self</span>.slot, <span class="ot">self</span>.slot + <span class="dv">1</span>

            <span class="ot">self</span>.commit(commit_slot, commit_proposal)

    <span class="kw">def</span> commit(<span class="ot">self</span>, slot, proposal):
        <span class="co">&quot;&quot;&quot;Actually commit a proposal that is decided and in sequence&quot;&quot;&quot;</span>
        decided_proposals = [p <span class="kw">for</span> s, p in <span class="ot">self</span>.decisions.iteritems() <span class="kw">if</span> s &lt; slot]
        <span class="kw">if</span> proposal in decided_proposals:
            <span class="ot">self</span>.logger.info(
                <span class="st">&quot;not committing duplicate proposal </span><span class="ot">%r</span><span class="st">, slot </span><span class="ot">%d</span><span class="st">&quot;</span>, proposal, slot)
            <span class="kw">return</span>  <span class="co"># duplicate</span>

        <span class="ot">self</span>.logger.info(<span class="st">&quot;committing </span><span class="ot">%r</span><span class="st"> at slot </span><span class="ot">%d</span><span class="st">&quot;</span> % (proposal, slot))
        <span class="kw">if</span> proposal.caller is not <span class="ot">None</span>:
            <span class="co"># perform a client operation</span>
            <span class="ot">self</span>.state, output = <span class="ot">self</span>.execute_fn(<span class="ot">self</span>.state, proposal.<span class="dt">input</span>)
            <span class="ot">self</span>.node.send([proposal.caller], 
                Invoked(client_id=proposal.client_id, output=output))

    <span class="co"># tracking the leader</span>

    <span class="kw">def</span> do_Adopted(<span class="ot">self</span>, sender, ballot_num, accepted_proposals):
        <span class="ot">self</span>.latest_leader = <span class="ot">self</span>.node.address
        <span class="ot">self</span>.leader_alive()

    <span class="kw">def</span> do_Accepting(<span class="ot">self</span>, sender, leader):
        <span class="ot">self</span>.latest_leader = leader
        <span class="ot">self</span>.leader_alive()

    <span class="kw">def</span> do_Active(<span class="ot">self</span>, sender):
        <span class="kw">if</span> sender != <span class="ot">self</span>.latest_leader:
            <span class="kw">return</span>
        <span class="ot">self</span>.leader_alive()

    <span class="kw">def</span> leader_alive(<span class="ot">self</span>):
        <span class="kw">if</span> <span class="ot">self</span>.latest_leader_timeout:
            <span class="ot">self</span>.latest_leader_timeout.cancel()

        <span class="kw">def</span> reset_leader():
            idx = <span class="ot">self</span>.peers.index(<span class="ot">self</span>.latest_leader)
            <span class="ot">self</span>.latest_leader = <span class="ot">self</span>.peers[(idx + <span class="dv">1</span>) % <span class="dt">len</span>(<span class="ot">self</span>.peers)]
            <span class="ot">self</span>.logger.debug(<span class="st">&quot;leader timed out; tring the next one, </span><span class="ot">%s</span><span class="st">&quot;</span>, 
                <span class="ot">self</span>.latest_leader)
        <span class="ot">self</span>.latest_leader_timeout = <span class="ot">self</span>.set_timer(LEADER_TIMEOUT, reset_leader)

    <span class="co"># adding new cluster members</span>

    <span class="kw">def</span> do_Join(<span class="ot">self</span>, sender):
        <span class="kw">if</span> sender in <span class="ot">self</span>.peers:
            <span class="ot">self</span>.node.send([sender], Welcome(
                state=<span class="ot">self</span>.state, slot=<span class="ot">self</span>.slot, decisions=<span class="ot">self</span>.decisions))</code></pre>

<h4 id="leader-scout-and-commander">Leader, Scout, and Commander</h4>

<p>The leader's primary task is to take <code>Propose</code> messages requesting new ballots and produce decisions. A leader is &quot;active&quot; when it has successfully carried out the <code>Prepare</code>/<code>Promise</code> portion of the protocol. An active leader can immediately send an <code>Accept</code> message in response to a <code>Propose</code>.</p>

<p>In keeping with the class-per-role model, the leader delegates to the scout and commander roles to carry out each portion of the protocol.</p>

<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">class</span> Leader(Role):

    <span class="kw">def</span> <span class="ot">__init__</span>(<span class="ot">self</span>, node, peers, commander_cls=Commander, scout_cls=Scout):
        <span class="dt">super</span>(Leader, <span class="ot">self</span>).<span class="ot">__init__</span>(node)
        <span class="ot">self</span>.ballot_num = Ballot(<span class="dv">0</span>, node.address)
        <span class="ot">self</span>.active = <span class="ot">False</span>
        <span class="ot">self</span>.proposals = {}
        <span class="ot">self</span>.commander_cls = commander_cls
        <span class="ot">self</span>.scout_cls = scout_cls
        <span class="ot">self</span>.scouting = <span class="ot">False</span>
        <span class="ot">self</span>.peers = peers

    <span class="kw">def</span> start(<span class="ot">self</span>):
        <span class="co"># reminder others we&#39;re active before LEADER_TIMEOUT expires</span>
        <span class="kw">def</span> active():
            <span class="kw">if</span> <span class="ot">self</span>.active:
                <span class="ot">self</span>.node.send(<span class="ot">self</span>.peers, Active())
            <span class="ot">self</span>.set_timer(LEADER_TIMEOUT / <span class="fl">2.0</span>, active)
        active()

    <span class="kw">def</span> spawn_scout(<span class="ot">self</span>):
        <span class="kw">assert</span> not <span class="ot">self</span>.scouting
        <span class="ot">self</span>.scouting = <span class="ot">True</span>
        <span class="ot">self</span>.scout_cls(<span class="ot">self</span>.node, <span class="ot">self</span>.ballot_num, <span class="ot">self</span>.peers).start()

    <span class="kw">def</span> do_Adopted(<span class="ot">self</span>, sender, ballot_num, accepted_proposals):
        <span class="ot">self</span>.scouting = <span class="ot">False</span>
        <span class="ot">self</span>.proposals.update(accepted_proposals)
        <span class="co"># note that we don&#39;t re-spawn commanders here; if there are undecided</span>
        <span class="co"># proposals, the replicas will re-propose</span>
        <span class="ot">self</span>.logger.info(<span class="st">&quot;leader becoming active&quot;</span>)
        <span class="ot">self</span>.active = <span class="ot">True</span>

    <span class="kw">def</span> spawn_commander(<span class="ot">self</span>, ballot_num, slot):
        proposal = <span class="ot">self</span>.proposals[slot]
        <span class="ot">self</span>.commander_cls(<span class="ot">self</span>.node, ballot_num, slot, proposal, <span class="ot">self</span>.peers).start()

    <span class="kw">def</span> do_Preempted(<span class="ot">self</span>, sender, slot, preempted_by):
        <span class="kw">if</span> not slot:  <span class="co"># from the scout</span>
            <span class="ot">self</span>.scouting = <span class="ot">False</span>
        <span class="ot">self</span>.logger.info(<span class="st">&quot;leader preempted by </span><span class="ot">%s</span><span class="st">&quot;</span>, preempted_by.leader)
        <span class="ot">self</span>.active = <span class="ot">False</span>
        <span class="ot">self</span>.ballot_num = Ballot((preempted_by or <span class="ot">self</span>.ballot_num).n + <span class="dv">1</span>, 
                                 <span class="ot">self</span>.ballot_num.leader)

    <span class="kw">def</span> do_Propose(<span class="ot">self</span>, sender, slot, proposal):
        <span class="kw">if</span> slot not in <span class="ot">self</span>.proposals:
            <span class="kw">if</span> <span class="ot">self</span>.active:
                <span class="ot">self</span>.proposals[slot] = proposal
                <span class="ot">self</span>.logger.info(<span class="st">&quot;spawning commander for slot </span><span class="ot">%d</span><span class="st">&quot;</span> % (slot,))
                <span class="ot">self</span>.spawn_commander(<span class="ot">self</span>.ballot_num, slot)
            <span class="kw">else</span>:
                <span class="kw">if</span> not <span class="ot">self</span>.scouting:
                    <span class="ot">self</span>.logger.info(<span class="st">&quot;got PROPOSE when not active - scouting&quot;</span>)
                    <span class="ot">self</span>.spawn_scout()
                <span class="kw">else</span>:
                    <span class="ot">self</span>.logger.info(<span class="st">&quot;got PROPOSE while scouting; ignored&quot;</span>)
        <span class="kw">else</span>:
            <span class="ot">self</span>.logger.info(<span class="st">&quot;got PROPOSE for a slot already being proposed&quot;</span>)</code></pre>

<p>The leader creates a scout role when it wants to become active, in response to receiving a <code>Propose</code> when it is inactive (<a href="clustering-by-consensus.html#figure-3.7">Figure 3.7</a>.) The scout sends (and re-sends, if necessary) a <code>Prepare</code> message, and collects <code>Promise</code> responses until it has heard from a majority of its peers or until it has been preempted. It communicates back to the leader with <code>Adopted</code> or <code>Preempted</code>, respectively. </p>

<div class="center figure">
<a name="figure-3.7"></a><img src="cluster-images/leaderscout.png" alt="Figure 3.7 - Scout" title="Figure 3.7 - Scout" />
</div>

<p class="center figcaption">
<small>Figure 3.7 - Scout</small>
</p>

<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">class</span> Scout(Role):

    <span class="kw">def</span> <span class="ot">__init__</span>(<span class="ot">self</span>, node, ballot_num, peers):
        <span class="dt">super</span>(Scout, <span class="ot">self</span>).<span class="ot">__init__</span>(node)
        <span class="ot">self</span>.ballot_num = ballot_num
        <span class="ot">self</span>.accepted_proposals = {}
        <span class="ot">self</span>.acceptors = <span class="dt">set</span>([])
        <span class="ot">self</span>.peers = peers
        <span class="ot">self</span>.quorum = <span class="dt">len</span>(peers) / <span class="dv">2</span> + <span class="dv">1</span>
        <span class="ot">self</span>.retransmit_timer = <span class="ot">None</span>

    <span class="kw">def</span> start(<span class="ot">self</span>):
        <span class="ot">self</span>.logger.info(<span class="st">&quot;scout starting&quot;</span>)
        <span class="ot">self</span>.send_prepare()

    <span class="kw">def</span> send_prepare(<span class="ot">self</span>):
        <span class="ot">self</span>.node.send(<span class="ot">self</span>.peers, Prepare(ballot_num=<span class="ot">self</span>.ballot_num))
        <span class="ot">self</span>.retransmit_timer = <span class="ot">self</span>.set_timer(PREPARE_RETRANSMIT, <span class="ot">self</span>.send_prepare)

    <span class="kw">def</span> update_accepted(<span class="ot">self</span>, accepted_proposals):
        acc = <span class="ot">self</span>.accepted_proposals
        <span class="kw">for</span> slot, (ballot_num, proposal) in accepted_proposals.iteritems():
            <span class="kw">if</span> slot not in acc or acc[slot][<span class="dv">0</span>] &lt; ballot_num:
                acc[slot] = (ballot_num, proposal)

    <span class="kw">def</span> do_Promise(<span class="ot">self</span>, sender, ballot_num, accepted_proposals):
        <span class="kw">if</span> ballot_num == <span class="ot">self</span>.ballot_num:
            <span class="ot">self</span>.logger.info(<span class="st">&quot;got matching promise; need </span><span class="ot">%d</span><span class="st">&quot;</span> % <span class="ot">self</span>.quorum)
            <span class="ot">self</span>.update_accepted(accepted_proposals)
            <span class="ot">self</span>.acceptors.add(sender)
            <span class="kw">if</span> <span class="dt">len</span>(<span class="ot">self</span>.acceptors) &gt;= <span class="ot">self</span>.quorum:
                <span class="co"># strip the ballot numbers from self.accepted_proposals, now that it</span>
                <span class="co"># represents a majority</span>
                accepted_proposals = \ 
                    <span class="dt">dict</span>((s, p) <span class="kw">for</span> s, (b, p) in <span class="ot">self</span>.accepted_proposals.iteritems())
                <span class="co"># We&#39;re adopted; note that this does *not* mean that no other</span>
                <span class="co"># leader is active.  # Any such conflicts will be handled by the</span>
                <span class="co"># commanders.</span>
                <span class="ot">self</span>.node.send([<span class="ot">self</span>.node.address],
                    Adopted(ballot_num=ballot_num, 
                            accepted_proposals=accepted_proposals))
                <span class="ot">self</span>.stop()
        <span class="kw">else</span>:
            <span class="co"># this acceptor has promised another leader a higher ballot number,</span>
            <span class="co"># so we&#39;ve lost</span>
            <span class="ot">self</span>.node.send([<span class="ot">self</span>.node.address], 
                Preempted(slot=<span class="ot">None</span>, preempted_by=ballot_num))
            <span class="ot">self</span>.stop()</code></pre>

<p>The leader creates a commander role for each slot where it has an active proposal (<a href="clustering-by-consensus.html#figure-3.8">Figure 3.8</a>.) Like a scout, a commander sends and re-sends <code>Accept</code> messages and waits for a majority of acceptors to reply with <code>Accepted</code>, or for news of its preemption. When a proposal is accepted, the commander broadcasts a <code>Decision</code> message to all nodes. It responds to the leader with <code>Decided</code> or <code>Preempted</code>.</p>

<div class="center figure">
<a name="figure-3.8"></a><img src="cluster-images/leadercommander.png" alt="Figure 3.8 - Commander" title="Figure 3.8 - Commander" />
</div>

<p class="center figcaption">
<small>Figure 3.8 - Commander</small>
</p>

<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">class</span> Commander(Role):

    <span class="kw">def</span> <span class="ot">__init__</span>(<span class="ot">self</span>, node, ballot_num, slot, proposal, peers):
        <span class="dt">super</span>(Commander, <span class="ot">self</span>).<span class="ot">__init__</span>(node)
        <span class="ot">self</span>.ballot_num = ballot_num
        <span class="ot">self</span>.slot = slot
        <span class="ot">self</span>.proposal = proposal
        <span class="ot">self</span>.acceptors = <span class="dt">set</span>([])
        <span class="ot">self</span>.peers = peers
        <span class="ot">self</span>.quorum = <span class="dt">len</span>(peers) / <span class="dv">2</span> + <span class="dv">1</span>

    <span class="kw">def</span> start(<span class="ot">self</span>):
        <span class="ot">self</span>.node.send(<span class="dt">set</span>(<span class="ot">self</span>.peers) - <span class="ot">self</span>.acceptors, Accept(
            slot=<span class="ot">self</span>.slot, ballot_num=<span class="ot">self</span>.ballot_num, proposal=<span class="ot">self</span>.proposal))
        <span class="ot">self</span>.set_timer(ACCEPT_RETRANSMIT, <span class="ot">self</span>.start)

    <span class="kw">def</span> finished(<span class="ot">self</span>, ballot_num, preempted):
        <span class="kw">if</span> preempted:
            <span class="ot">self</span>.node.send([<span class="ot">self</span>.node.address], 
                           Preempted(slot=<span class="ot">self</span>.slot, preempted_by=ballot_num))
        <span class="kw">else</span>:
            <span class="ot">self</span>.node.send([<span class="ot">self</span>.node.address], 
                           Decided(slot=<span class="ot">self</span>.slot))
        <span class="ot">self</span>.stop()

    <span class="kw">def</span> do_Accepted(<span class="ot">self</span>, sender, slot, ballot_num):
        <span class="kw">if</span> slot != <span class="ot">self</span>.slot:
            <span class="kw">return</span>
        <span class="kw">if</span> ballot_num == <span class="ot">self</span>.ballot_num:
            <span class="ot">self</span>.acceptors.add(sender)
            <span class="kw">if</span> <span class="dt">len</span>(<span class="ot">self</span>.acceptors) &lt; <span class="ot">self</span>.quorum:
                <span class="kw">return</span>
            <span class="ot">self</span>.node.send(<span class="ot">self</span>.peers, Decision(
                           slot=<span class="ot">self</span>.slot, proposal=<span class="ot">self</span>.proposal))
            <span class="ot">self</span>.finished(ballot_num, <span class="ot">False</span>)
        <span class="kw">else</span>:
            <span class="ot">self</span>.finished(ballot_num, <span class="ot">True</span>)</code></pre>

<p>As an aside, a surprisingly subtle bug appeared here during development. At the time, the network simulator introduced packet loss even on messages within a node. When <em>all</em> <code>Decision</code> messages were lost, the protocol could not proceed. The replica continued to re-transmit <code>Propose</code> messages, but the leader ignored them as it already had a proposal for that slot. The replica's catch-up process could not find the result, as no replica had heard of the decision. The solution was to ensure that local messages are always delivered, as is the case for real network stacks.</p>

<h4 id="bootstrap">Bootstrap</h4>

<p>When a node joins the cluster, it must determine the current cluster state before it can participate. The bootstrap role handles this by sending <code>Join</code> messages to each peer in turn until it receives a <code>Welcome</code>. Bootstrap's communication diagram is shown above in <a href="clustering-by-consensus.html#sec.cluster.replica">Replica</a>.</p>

<p>An early version of the implementation started each node with a full set of roles (replica, leader, and acceptor), each of which began in a &quot;startup&quot; phase, waiting for information from the <code>Welcome</code> message. This spread the initialization logic around every role, requiring separate testing of each one. The final design has the bootstrap role adding each of the other roles to the node once startup is complete, passing the initial state to their constructors.</p>

<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">class</span> Bootstrap(Role):

    <span class="kw">def</span> <span class="ot">__init__</span>(<span class="ot">self</span>, node, peers, execute_fn,
                 replica_cls=Replica, acceptor_cls=Acceptor, leader_cls=Leader,
                 commander_cls=Commander, scout_cls=Scout):
        <span class="dt">super</span>(Bootstrap, <span class="ot">self</span>).<span class="ot">__init__</span>(node)
        <span class="ot">self</span>.execute_fn = execute_fn
        <span class="ot">self</span>.peers = peers
        <span class="ot">self</span>.peers_cycle = itertools.cycle(peers)
        <span class="ot">self</span>.replica_cls = replica_cls
        <span class="ot">self</span>.acceptor_cls = acceptor_cls
        <span class="ot">self</span>.leader_cls = leader_cls
        <span class="ot">self</span>.commander_cls = commander_cls
        <span class="ot">self</span>.scout_cls = scout_cls

    <span class="kw">def</span> start(<span class="ot">self</span>):
        <span class="ot">self</span>.join()

    <span class="kw">def</span> join(<span class="ot">self</span>):
        <span class="ot">self</span>.node.send([<span class="dt">next</span>(<span class="ot">self</span>.peers_cycle)], Join())
        <span class="ot">self</span>.set_timer(JOIN_RETRANSMIT, <span class="ot">self</span>.join)

    <span class="kw">def</span> do_Welcome(<span class="ot">self</span>, sender, state, slot, decisions):
        <span class="ot">self</span>.acceptor_cls(<span class="ot">self</span>.node)
        <span class="ot">self</span>.replica_cls(<span class="ot">self</span>.node, execute_fn=<span class="ot">self</span>.execute_fn, peers=<span class="ot">self</span>.peers,
                         state=state, slot=slot, decisions=decisions)
        <span class="ot">self</span>.leader_cls(<span class="ot">self</span>.node, peers=<span class="ot">self</span>.peers, commander_cls=<span class="ot">self</span>.commander_cls,
                        scout_cls=<span class="ot">self</span>.scout_cls).start()
        <span class="ot">self</span>.stop()</code></pre>

<h4 id="seed">Seed</h4>

<p>In normal operation, when a node joins the cluster, it expects to find the cluster already running, with at least one node willing to respond to a <code>Join</code> message. But how does the cluster get started? One option is for the bootstrap role to determine, after attempting to contact every other node, that it is the first in the cluster. But this has two problems. First, for a large cluster it means a long wait while each <code>Join</code> times out. More importantly, in the event of a network partition, a new node might be unable to contact any others and start a new cluster.</p>

<p>Network partitions are the most challenging failure case for clustered applications. In a network partition, all cluster members remain alive, but communication fails between some members. For example, if the network link joining a cluster with nodes in Berlin and Taipei fails, the network is partitioned. If both parts of a cluster continue to operate during a partition, then re-joining the parts after the network link is restored can be challenging. In the Multi-Paxos case, the healed network would be hosting two clusters with different decisions for the same slot numbers.</p>

<p>To avoid this outcome, creating a new cluster is a user-specified operation. Exactly one node in the cluster runs the seed role, with the others running bootstrap as usual. The seed waits until it has received <code>Join</code> messages from a majority of its peers, then sends a <code>Welcome</code> with an initial state for the state machine and an empty set of decisions. The seed role then stops itself and starts a bootstrap role to join the newly-seeded cluster.</p>

<p>Seed emulates the <code>Join</code>/<code>Welcome</code> part of the bootstrap/replica interaction, so its communication diagram is the same as for the replica role.</p>

<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">class</span> Seed(Role):

    <span class="kw">def</span> <span class="ot">__init__</span>(<span class="ot">self</span>, node, initial_state, execute_fn, peers, 
                 bootstrap_cls=Bootstrap):
        <span class="dt">super</span>(Seed, <span class="ot">self</span>).<span class="ot">__init__</span>(node)
        <span class="ot">self</span>.initial_state = initial_state
        <span class="ot">self</span>.execute_fn = execute_fn
        <span class="ot">self</span>.peers = peers
        <span class="ot">self</span>.bootstrap_cls = bootstrap_cls
        <span class="ot">self</span>.seen_peers = <span class="dt">set</span>([])
        <span class="ot">self</span>.exit_timer = <span class="ot">None</span>

    <span class="kw">def</span> do_Join(<span class="ot">self</span>, sender):
        <span class="ot">self</span>.seen_peers.add(sender)
        <span class="kw">if</span> <span class="dt">len</span>(<span class="ot">self</span>.seen_peers) &lt;= <span class="dt">len</span>(<span class="ot">self</span>.peers) / <span class="dv">2</span>:
            <span class="kw">return</span>

        <span class="co"># cluster is ready - welcome everyone</span>
        <span class="ot">self</span>.node.send(<span class="dt">list</span>(<span class="ot">self</span>.seen_peers), Welcome(
            state=<span class="ot">self</span>.initial_state, slot=<span class="dv">1</span>, decisions={}))

        <span class="co"># stick around for long enough that we don&#39;t hear any new JOINs from</span>
        <span class="co"># the newly formed cluster</span>
        <span class="kw">if</span> <span class="ot">self</span>.exit_timer:
            <span class="ot">self</span>.exit_timer.cancel()
        <span class="ot">self</span>.exit_timer = <span class="ot">self</span>.set_timer(JOIN_RETRANSMIT * <span class="dv">2</span>, <span class="ot">self</span>.finish)

    <span class="kw">def</span> finish(<span class="ot">self</span>):
        <span class="co"># bootstrap this node into the cluster we just seeded</span>
        bs = <span class="ot">self</span>.bootstrap_cls(<span class="ot">self</span>.node, 
                                peers=<span class="ot">self</span>.peers, execute_fn=<span class="ot">self</span>.execute_fn)
        bs.start()
        <span class="ot">self</span>.stop()</code></pre>

<h4 id="requester">Requester</h4>

<p>The requester role manages a request to the distributed state machine. The role class simply sends <code>Invoke</code> messages to the local replica until it receives a corresponding <code>Invoked</code>. See the &quot;Replica&quot; section, above, for this role's communication diagram.</p>

<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">class</span> Requester(Role):

    client_ids = itertools.count(start=<span class="dv">100000</span>)

    <span class="kw">def</span> <span class="ot">__init__</span>(<span class="ot">self</span>, node, n, callback):
        <span class="dt">super</span>(Requester, <span class="ot">self</span>).<span class="ot">__init__</span>(node)
        <span class="ot">self</span>.client_id = <span class="ot">self</span>.client_ids.<span class="dt">next</span>()
        <span class="ot">self</span>.n = n
        <span class="ot">self</span>.output = <span class="ot">None</span>
        <span class="ot">self</span>.callback = callback

    <span class="kw">def</span> start(<span class="ot">self</span>):
        <span class="ot">self</span>.node.send([<span class="ot">self</span>.node.address], 
                       Invoke(caller=<span class="ot">self</span>.node.address, 
                              client_id=<span class="ot">self</span>.client_id, input_value=<span class="ot">self</span>.n))
        <span class="ot">self</span>.invoke_timer = <span class="ot">self</span>.set_timer(INVOKE_RETRANSMIT, <span class="ot">self</span>.start)

    <span class="kw">def</span> do_Invoked(<span class="ot">self</span>, sender, client_id, output):
        <span class="kw">if</span> client_id != <span class="ot">self</span>.client_id:
            <span class="kw">return</span>
        <span class="ot">self</span>.logger.debug(<span class="st">&quot;received output </span><span class="ot">%r</span><span class="st">&quot;</span> % (output,))
        <span class="ot">self</span>.invoke_timer.cancel()
        <span class="ot">self</span>.callback(output)
        <span class="ot">self</span>.stop()</code></pre>

<h3 id="summary">Summary</h3>

<p>To recap, cluster's roles are:</p>

<ul>
<li>Acceptor -- make promises and accept proposals</li>
<li>Replica -- manage the distributed state machine: submitting proposals, committing decisions, and responding to requesters</li>
<li>Leader -- lead rounds of the Multi-Paxos algorithm</li>
<li>Scout -- perform the <code>Prepare</code>/<code>Promise</code> portion of the Multi-Paxos algorithm for a leader</li>
<li>Commander -- perform the <code>Accept</code>/<code>Accepted</code> portion of the Multi-Paxos algorithm for a leader</li>
<li>Bootstrap -- introduce a new node to an existing cluster</li>
<li>Seed -- create a new cluster</li>
<li>Requester -- request a distributed state machine operation</li>
</ul>

<p>There is just one more piece of equipment required to make Cluster go: the network through which all of the nodes communicate.</p>

<h2 id="network">Network</h2>

<p>Any network protocol needs the ability to send and receive messages and a means of calling functions at a time in the future.</p>

<p>The <code>Network</code> class provides a simple simulated network with these capabilities and also simulates packet loss and message propagation delays.</p>

<p>Timers are handled using Python's <code>heapq</code> module, allowing efficient selection of the next event. Setting a timer involves pushing a <code>Timer</code> object onto the heap. Since removing items from a heap is inefficient, cancelled timers are left in place but marked as cancelled.</p>

<p>Message transmission uses the timer functionality to schedule a later delivery of the message at each node, using a random simulated delay. We again use <code>functools.partial</code> to set up a future call to the destination node's <code>receive</code> method with appropriate arguments.</p>

<p>Running the simulation just involves popping timers from the heap and executing them if they have not been cancelled and if the destination node is still active.</p>

<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">class</span> Timer(<span class="dt">object</span>):

    <span class="kw">def</span> <span class="ot">__init__</span>(<span class="ot">self</span>, expires, address, callback):
        <span class="ot">self</span>.expires = expires
        <span class="ot">self</span>.address = address
        <span class="ot">self</span>.callback = callback
        <span class="ot">self</span>.cancelled = <span class="ot">False</span>

    <span class="kw">def</span> <span class="ot">__cmp__</span>(<span class="ot">self</span>, other):
        <span class="kw">return</span> <span class="dt">cmp</span>(<span class="ot">self</span>.expires, other.expires)

    <span class="kw">def</span> cancel(<span class="ot">self</span>):
        <span class="ot">self</span>.cancelled = <span class="ot">True</span>


<span class="kw">class</span> Network(<span class="dt">object</span>):
    PROP_DELAY = <span class="fl">0.03</span>
    PROP_JITTER = <span class="fl">0.02</span>
    DROP_PROB = <span class="fl">0.05</span>

    <span class="kw">def</span> <span class="ot">__init__</span>(<span class="ot">self</span>, seed):
        <span class="ot">self</span>.nodes = {}
        <span class="ot">self</span>.rnd = random.Random(seed)
        <span class="ot">self</span>.timers = []
        <span class="ot">self</span>.now = <span class="fl">1000.0</span>

    <span class="kw">def</span> new_node(<span class="ot">self</span>, address=<span class="ot">None</span>):
        node = Node(<span class="ot">self</span>, address=address)
        <span class="ot">self</span>.nodes[node.address] = node
        <span class="kw">return</span> node

    <span class="kw">def</span> run(<span class="ot">self</span>):
        <span class="kw">while</span> <span class="ot">self</span>.timers:
            next_timer = <span class="ot">self</span>.timers[<span class="dv">0</span>]
            <span class="kw">if</span> next_timer.expires &gt; <span class="ot">self</span>.now:
                <span class="ot">self</span>.now = next_timer.expires
            heapq.heappop(<span class="ot">self</span>.timers)
            <span class="kw">if</span> next_timer.cancelled:
                <span class="kw">continue</span>
            <span class="kw">if</span> not next_timer.address or next_timer.address in <span class="ot">self</span>.nodes:
                next_timer.callback()

    <span class="kw">def</span> stop(<span class="ot">self</span>):
        <span class="ot">self</span>.timers = []

    <span class="kw">def</span> set_timer(<span class="ot">self</span>, address, seconds, callback):
        timer = Timer(<span class="ot">self</span>.now + seconds, address, callback)
        heapq.heappush(<span class="ot">self</span>.timers, timer)
        <span class="kw">return</span> timer

    <span class="kw">def</span> send(<span class="ot">self</span>, sender, destinations, message):
        sender.logger.debug(<span class="st">&quot;sending </span><span class="ot">%s</span><span class="st"> to </span><span class="ot">%s</span><span class="st">&quot;</span>, message, destinations)
        <span class="co"># avoid aliasing by making a closure containing distinct deep copy of</span>
        <span class="co"># message for each dest</span>
        <span class="kw">def</span> sendto(dest, message):
            <span class="kw">if</span> dest == sender.address:
                <span class="co"># reliably deliver local messages with no delay</span>
                <span class="ot">self</span>.set_timer(sender.address, <span class="dv">0</span>,  
                               <span class="kw">lambda</span>: sender.receive(sender.address, message))
            <span class="kw">elif</span> <span class="ot">self</span>.rnd.uniform(<span class="dv">0</span>, <span class="fl">1.0</span>) &gt; <span class="ot">self</span>.DROP_PROB:
                delay = <span class="ot">self</span>.PROP_DELAY + <span class="ot">self</span>.rnd.uniform(-<span class="ot">self</span>.PROP_JITTER, 
                                                           <span class="ot">self</span>.PROP_JITTER)
                <span class="ot">self</span>.set_timer(dest, delay, 
                               functools.partial(<span class="ot">self</span>.nodes[dest].receive, 
                                                 sender.address, message))
        <span class="kw">for</span> dest in (d <span class="kw">for</span> d in destinations <span class="kw">if</span> d in <span class="ot">self</span>.nodes):
            sendto(dest, copy.deepcopy(message))</code></pre>

<p>While it's not included in this implementation, the component model allows us to swap in a real-world network implementation, communicating between actual servers on a real network, with no changes to the other components. Testing and debugging can take place using the simulated network, with production use of the library operating over real network hardware.</p>

<h2 id="debugging-support">Debugging Support</h2>

<p>When developing a complex system such as this, the bugs quickly transition from trivial, like a simple <code>NameError</code>, to obscure failures that only manifest after several minutes of (simulated) proocol operation. Chasing down bugs like this involves working backward from the point where the error became obvious. Interactive debuggers are useless here, as they can only step forward in time.</p>

<p>The most important debugging feature in Cluster is a <em>deterministic</em> simulator. Unlike a real network, it will behave exactly the same way on every run, given the same seed for the random number generator. This means that we can add additional debugging checks or output to the code and re-run the simulation to see the same failure in more detail.</p>

<p>Of course, much of that detail is in the messages exchanged by the nodes in the cluster, so those are automatically logged in their entirety. That logging includes the role class sending or receiving the message, as well as the simulated timestamp injected via the <code>SimTimeLogger</code> class.</p>

<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">class</span> SimTimeLogger(logging.LoggerAdapter):

    <span class="kw">def</span> process(<span class="ot">self</span>, msg, kwargs):
        <span class="kw">return</span> <span class="st">&quot;T=</span><span class="ot">%.3f</span><span class="st"> </span><span class="ot">%s</span><span class="st">&quot;</span> % (<span class="ot">self</span>.extra[<span class="st">&#39;network&#39;</span>].now, msg), kwargs

    <span class="kw">def</span> getChild(<span class="ot">self</span>, name):
        <span class="kw">return</span> <span class="ot">self</span>.__class__(<span class="ot">self</span>.logger.getChild(name),
                              {<span class="st">&#39;network&#39;</span>: <span class="ot">self</span>.extra[<span class="st">&#39;network&#39;</span>]})</code></pre>

<p>A resilient protocol such as this one can often run for a long time after a bug has been triggered. For example, during development, a data aliasing error caused all replicas to share the same <code>decisions</code> dictionary. This meant that once a decision was handled on one node, all other nodes saw it as already decided. Even with this serious bug, the cluster produced correct results for several transactions before deadlocking.</p>

<p>Assertions are an important tool to catch this sort of error early. Assertions should include any invariants from the algorithm design, but when the code doesn't behave as we expect, asserting our expectations is a great way to see where things go astray.</p>

<pre class="sourceCode python"><code class="sourceCode python">    <span class="kw">assert</span> not <span class="ot">self</span>.decisions.get(<span class="ot">self</span>.slot, <span class="ot">None</span>), \
            <span class="co">&quot;next slot to commit is already decided&quot;</span>
    <span class="kw">if</span> slot in <span class="ot">self</span>.decisions:
        <span class="kw">assert</span> <span class="ot">self</span>.decisions[slot] == proposal, \
            <span class="co">&quot;slot %d already decided with %r!&quot;</span> % (slot, <span class="ot">self</span>.decisions[slot])</code></pre>

<p>Identifying the right assumptions we make while reading code is a part of the art of debugging. In this code from <code>Replica.do_Decision</code>, the problem was that the <code>Decision</code> for the next slot to commit was being ignored because it was already in <code>self.decisions</code>. The underlying assumption being violated was that the next slot to be committed was not yet decided. Asserting this at the beginning of <code>do_Decision</code> identified the flaw and led quickly to the fix. Similarly, other bugs led to cases where different proposals were decided in the same slot -- a serious error.</p>

<p>Many other assertions were added during development of the protocol, but in the interests of space, only a few remain.</p>

<h2 id="testing">Testing</h2>

<p>Some time in the last ten years, coding without tests finally became as crazy as driving without a seatbelt. Code without tests is probably incorrect, and modifying code is risky without a way to see if its behavior has changed.</p>

<p>Testing is most effective when the code is organized for testability. There are a few active schools of thought in this area, but the approach we've taken is to divide the code into small, minimally connected units that can be tested in isolation. This agrees nicely with the role model, where each role has a specific purpose and can operate in isolation from the others, resulting in a compact, self-sufficient class.</p>

<p>Cluster is written to maximize that isolation: all communication between roles takes place via messages, with the exception of creating new roles. For the most part, then, roles can be tested by sending messages to them and observing their responses.</p>

<h4 id="unit-testing">Unit Testing</h4>

<p>The unit tests for Cluster are simple and short:</p>

<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">class</span> Tests(utils.ComponentTestCase):
    <span class="kw">def</span> test_propose_active(<span class="ot">self</span>):
        <span class="co">&quot;&quot;&quot;A PROPOSE received while active spawns a commander.&quot;&quot;&quot;</span>
        <span class="ot">self</span>.activate_leader()
        <span class="ot">self</span>.node.fake_message(Propose(slot=<span class="dv">10</span>, proposal=PROPOSAL1))
        <span class="ot">self</span>.assertCommanderStarted(Ballot(<span class="dv">0</span>, <span class="st">&#39;F999&#39;</span>), <span class="dv">10</span>, PROPOSAL1)</code></pre>

<p>This method tests a single behavior (commander spawning) of a single unit (the <code>Leader</code> class). It follows the well-known &quot;arrange, act, assert&quot; pattern: set up an active leader, send it a message, and check the result.</p>

<h4 id="dependency-injection">Dependency Injection</h4>

<p>We use a technique called &quot;dependency injection&quot; to handle creation of new roles. Each role class which adds other roles to the network takes a list of class objects as constructor arguments, defaulting to the actual classes. For example, the constructor for <code>Leader</code> looks like this:</p>

<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">class</span> Leader(Role):
    <span class="kw">def</span> <span class="ot">__init__</span>(<span class="ot">self</span>, node, peers, commander_cls=Commander, scout_cls=Scout):
        <span class="dt">super</span>(Leader, <span class="ot">self</span>).<span class="ot">__init__</span>(node)
        <span class="ot">self</span>.ballot_num = Ballot(<span class="dv">0</span>, node.address)
        <span class="ot">self</span>.active = <span class="ot">False</span>
        <span class="ot">self</span>.proposals = {}
        <span class="ot">self</span>.commander_cls = commander_cls
        <span class="ot">self</span>.scout_cls = scout_cls
        <span class="ot">self</span>.scouting = <span class="ot">False</span>
        <span class="ot">self</span>.peers = peers</code></pre>

<p>The <code>spawn_scout</code> method (and similarly, <code>spawn_commander</code>) creates the new role object with <code>self.scout_cls</code>:</p>

<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">class</span> Leader(Role):
    <span class="kw">def</span> spawn_scout(<span class="ot">self</span>):
        <span class="kw">assert</span> not <span class="ot">self</span>.scouting
        <span class="ot">self</span>.scouting = <span class="ot">True</span>
        <span class="ot">self</span>.scout_cls(<span class="ot">self</span>.node, <span class="ot">self</span>.ballot_num, <span class="ot">self</span>.peers).start()</code></pre>

<p>The magic of this technique is that, in testing, <code>Leader</code> can be given fake classes and thus tested separately from <code>Scout</code> and <code>Commander</code>.</p>

<h4 id="interface-correctness">Interface Correctness</h4>

<p>One pitfall of a focus on small units is that it does not test the interfaces between units. For example, unit tests for the acceptor role verify the format of the <code>accepted</code> attribute of the <code>Promise</code> message, and the unit tests for the scout role supply well-formatted values for the attribute. Neither test checks that those formats match.</p>

<p>One approach to fixing this issue is to make the interfaces self-enforcing. In Cluster, the use of named tuples and keyword arguments avoids any disagreement over messages' attributes. Because the only interaction between role classes is via messages, this covers a large part of the interface.</p>

<p>For specific issues such as the format of <code>accepted_proposals</code>, both the real and test data can be verified using the same function, in this case <code>verifyPromiseAccepted</code>. The tests for the acceptor use this method to verify each returned <code>Promise</code>, and the tests for the scout use it to verify every fake <code>Promise</code>.</p>

<h4 id="integration-testing">Integration Testing</h4>

<p>The final bulwark against interface problems and design errors is integration testing. An integration test assembles multiple units together and tests their combined effect. In our case, that means building a network of several nodes, injecting some requests into it, and verifying the results. If there are any interface issues not discovered in unit testing, they should cause the integration tests to fail quickly.</p>

<p>Because the protocol is intended to handle node failure gracefully, we test a few failure scenarios as well, including the untimely failure of the active leader.</p>

<p>Integration tests are harder to write than unit tests, because they are less well-isolated. For Cluster, this is clearest in testing the failed leader, as any node could be the active leader. Even with a deterministic network, a change in one message alters the random number generator's state and thus unpredictably changes later events. Rather than hard-coding the expected leader, the test code must dig into the internal state of each leader to find one that believes itself to be active.</p>

<h4 id="fuzz-testing">Fuzz Testing</h4>

<p>It's very difficult to test resilient code: it is likely to be resilient to its own bugs, so integration tests may not detect even very serious bugs. It is also hard to imagine and construct tests for every possible failure mode.</p>

<p>A common approach to this sort of problem is &quot;fuzz testing&quot;: running the code repeatedly with randomly changing inputs until something breaks. When something <em>does</em> break, all of the debugging support becomes critical: if the failure can't be reproduced, and the logging information isn't sufficient to find the bug, then you can't fix it!</p>

<p>I performed some manual fuzz testing of cluster during development, but a full fuzz-testing infrastructure is beyond the scope of this project.</p>

<h2 id="power-struggles">Power Struggles</h2>

<p>A cluster with many active leaders is a very noisy place, with scouts sending ever-increasing ballot numbers to acceptors, and no ballots being decided. A cluster with no active leader is quiet, but equally nonfunctional. Balancing the implementation so that a cluster almost always agrees on exactly one leader is remarkably difficult.</p>

<p>It's easy enough to avoid fighting leaders: when preempted, a leader just accepts its new inactive status. However, this easily leads to a case where there are no active leaders, so an inactive leader will try to become active every time it gets a <code>Propose</code> message.</p>

<p>If the whole cluster doesn't agree on which member is the active leader, there's trouble: different replicas send <code>Propose</code> messages to different leaders, leading to battling scouts. So it's important that leader elections be decided quickly, and that all cluster members find out about the result as quickly as possible.</p>

<p>Cluster handles this by detecting a leader change as quickly as possible: when an acceptor sends a <code>Promise</code>, chances are good that the promised member will be the next leader. Failures are detected with a heartbeat protocol.</p>

<h2 id="further-extensions">Further Extensions</h2>

<p>Of course, there are plenty of ways we could extend and improve this implementation.</p>

<h3 id="catching-up">Catching Up</h3>

<p>In &quot;pure&quot; Multi-Paxos, nodes which fail to receive messages can be many slots behind the rest of the cluster. As long as the state of the distributed state machine is never accessed except via state machine transitions, this design is functional. To read from the state, the client requests a state-machine transition that does not actually alter the state, but which returns the desired value. This transition is executed cluster-wide, ensuring that it returns the same value everywhere, based on the state at the slot in which it is proposed.</p>

<p>Even in the optimal case, this is slow, requiring several round trips just to read a value. If a distributed object store made such a request for every object access, its performance would be dismal. But when the node receiving the request is lagging behind, the request delay is much greater as that node must catch up to the rest of the cluster before making a successful proposal.</p>

<p>A simple solution is to implement a gossip-style protocol, where each replica periodically contacts other replicas to share the highest slot it knows about and to request information on unknown slots. Then even when a <code>Decision</code> message was lost, the replica would quickly find out about the decision from one of its peers.</p>

<h3 id="consistent-memory-usage">Consistent Memory Usage</h3>

<p>A cluster-management library provides reliability in the presence of unreliable components. It shouldn't add unreliability of its own. Unfortunately, Cluster will not run for long without failing due to ever-growing memory use and message size.</p>

<p>In the protocol definition, acceptors and replicas form the &quot;memory&quot; of the protocol, so they need to remember everything. These classes never know when they will receive a request for an old slot, perhaps from a lagging replica or leader. To maintain correctness, then, they keep a list of every decision, ever, since the cluster was started. Worse, these decisions are transmitted between replicas in <code>Welcome</code> messages, making these messages enormous in a long-lived cluster.</p>

<p>One technique to address this issue is to periodically &quot;checkpoint&quot; each node's state, keeping information about some limited number of decisions on hand. Nodes which are so out of date that they have not committed all slots up to the checkpoint must &quot;reset&quot; themselves by leaving and re-joining the cluster.</p>

<h4 id="persistent-storage">Persistent Storage</h4>

<p>While it's OK for a minority of cluster members to fail, it's not OK for an acceptor to &quot;forget&quot; any of the values it has accepted or promises it has made.</p>

<p>Unfortunately, this is exactly what happens when a cluster member fails and restarts: the newly initialized Acceptor instance has no record of the promises its predecessor made. The problem is that the newly-started instance takes the place of the old.</p>

<p>There are two ways to solve this issue. The simpler solution involves writing acceptor state to disk and re-reading that state on startup. The more complex solution is to remove failed cluster members from the cluster, and require that new members be added to the cluster. This kind of dynamic adjustment of the cluster membership is called a &quot;view change&quot;.</p>

<h4 id="view-changes">View Changes</h4>

<p>Operations engineers need to be able to resize clusters to meet load and availability requirements. A simple test project might begin with a minimal cluster of three nodes, where any one can fail without impact. When that project goes &quot;live&quot;, though, the additional load will require a larger cluster.</p>

<p>Cluster, as written, cannot change the set of peers in a cluster without restarting the entire cluster. Ideally, the cluster would be able to maintain a consensus about its membership, just as it does about state machine transitions. This means that the set of cluster members (the <em>view</em>) can be changed by special view-change proposals. But the Paxos algorithm depends on universal agreement about the members in the cluster, so we must define the view for each slot.</p>

<p>Lamport addresses this challenge in the final paragraph of &quot;Paxos Made Simple&quot;:</p>

<blockquote>
<p>We can allow a leader to get <span class="math">\(\alpha\)</span> commands ahead by letting the set of servers that execute instance <span class="math">\(i+\alpha\)</span> of the consensus algorithm be specified by the state after execution of the <span class="math">\(i\)</span>th state machine command. (Lamport, 2001)</p>
</blockquote>

<p>The idea is that each instance of Paxos (slot) uses the view from <span class="math">\(\alpha\)</span> slots earlier. This allows the cluster to work on, at most, <span class="math">\(\alpha\)</span> slots at any one time, so a very small value of <span class="math">\(\alpha\)</span> limits concurrency, while a very large value of <span class="math">\(\alpha\)</span> makes view changes slow to take effect.</p>

<p>In early drafts of this implementation (dutifully preserved in the git history!), I implemented support for view changes (using <span class="math">\(\alpha\)</span> in place of 3). This seemingly simple change introduced a great deal of complexity:</p>

<ul>
<li>tracking the view for each of the last <span class="math">\(\alpha\)</span> committed slots and correctly sharing this with new nodes,</li>
<li>ignoring proposals for which no slot is available,</li>
<li>detecting failed nodes,</li>
<li>properly serializing multiple competing view changes, and</li>
<li>communicating view information between the leader and replica.</li>
</ul>

<p>The result was far too large for this book! </p>

<h2 id="references">References</h2>

<p>In addition to the original Paxos paper and Lamport's follow-up &quot;Paxos Made Simple&quot;<a href="clustering-by-consensus.html#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>, our implementation added extensions that were informed by several other resources. The role names were taken from &quot;Paxos Made Moderately Complex&quot;<a href="clustering-by-consensus.html#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>. &quot;Paxos Made Live&quot;<a href="clustering-by-consensus.html#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> was helpful regarding snapshots in particular, and <a href="http://www.scs.stanford.edu/~dm/home/papers/paxos.pdf">&quot;Paxos Made Practical&quot;</a> described view changes (although not of the type described here.) Liskov's &quot;From Viewstamped Replication to Byzantine Fault Tolerance&quot;<a href="clustering-by-consensus.html#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> provided yet another perspective on view changes. Finally, a <a href="http://stackoverflow.com/questions/21353312/in-part-time-parliament-why-does-using-the-membership-from-decree-n-3-work-to">Stack Overflow discussion</a> was helpful in learning how members are added and removed from the system.</p>

<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>L. Lamport, &quot;The Part-Time Parliament,&quot; ACM Transactions on Computer Systems, 16(2):133–169, May 1998.<a href="clustering-by-consensus.html#fnref1">↩</a></p></li>
<li id="fn2"><p>L. Lamport, &quot;Paxos Made Simple,&quot; ACM SIGACT News (Distributed Computing Column) 32, 4 (Whole Number 121, December 2001) 51-58.<a href="clustering-by-consensus.html#fnref2">↩</a></p></li>
<li id="fn3"><p>R. Van Renesse and D. Altinbuken, &quot;Paxos Made Moderately Complex,&quot; ACM Comp. Survey 47, 3, Article 42 (Feb. 2015)<a href="clustering-by-consensus.html#fnref3">↩</a></p></li>
<li id="fn4"><p>T. Chandra, R. Griesemer, and J. Redstone, &quot;Paxos Made Live - An Engineering Perspective,&quot; Proceedings of the twenty-sixth annual ACM symposium on Principles of distributed computing (PODC '07). ACM, New York, NY, USA, 398-407.<a href="clustering-by-consensus.html#fnref4">↩</a></p></li>
<li id="fn5"><p>B. Liskov, &quot;From Viewstamped Replication to Byzantine Fault Tolerance,&quot; In <em>Replication</em>, Springer-Verlag, Berlin, Heidelberg 121-149 (2010)<a href="clustering-by-consensus.html#fnref5">↩</a></p></li>
</ol>
</div>
        </div>
      </div>
    </div>
  </body>
</html>